% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bussproofs}
\usepackage{cite}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Surprising Pattern Mining in Hypergraph as a Form of Reasoning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Nil Geisweiller
  %\orcidID{0000-0001-5041-6299}
  \and Ben Goertzel}
%
\authorrunning{N. Geisweiller et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{ SingularityNET Foundation, The
  Netherlands\\ \email{\{nil,ben\}@singularitynet.io}}
%
\maketitle              % typeset the header of the contribution
%

\begin{abstract}
  In this paper we introduce a pattern miner algorithm alongside a
  definition of surprisingness. Both the algorithm and the
  surprisingness measure are framed as a reasoning, conferring some
  advantages, in particular that of being able to handle a very broad
  definition of surprisingness. The work is presented in the context
  of its implementation over the OpenCog framework.

  \keywords{Pattern Miner \and Surprisingness \and Reasoning.}
\end{abstract}

\section{Introduction}

Finding patterns is a way of discovering knowledge and learning about
the data and the world they represent. These patterns can then be used
in various ways like constructing predictive models to help to act in
this world \cite{Jade12Pat}, or to be passed on to human
expertise. Pattern mining algorithms tend to work at the
\emph{syntactic} level, such as subtree mining \cite{Chi2005Freq},
where patterns are subtrees within a database of trees, and each
subtree represents a concept containing all the trees compatible with
that subtree. This is both a limit and a strength. Limit because they
cannot express arbitrary abstractions, and strength because for that
reason they can be relatively efficient. Moreover even purely
syntactic pattern miners can go a long way if much of the semantic
knowledge is turned into syntax. For instance if the data contains
$$\texttt{human(John)}$$
$$\texttt{human}\Rightarrow\texttt{mortal}$$ a purely syntactic
pattern miner will not be able to take into account the implicit
datum $$\texttt{mortal(John)}$$ unless a step of inference is formerly
taken to make it visible.

Another shortcoming of pattern mining is the volume of patterns it
tend to produce. For that reason it can be useful to rank the patterns
according to some measure of interestingness \cite{Vreeken2014}.

\subsection{Contribution}

In this paper we introduce a pattern miner algorithm to find patterns
in hypergraph database alongside a measure of surprisingness. Both are
implemented in the OpenCog framework \cite{Goertzel2014}, on top of the
\emph{Unified Rule Engine}, URE for short, the reasoning engine of
OpenCog. Framing pattern mining as reasoning provides the following
advantages:
\begin{enumerate}
\item Enable hybridizations between syntactic and semantic pattern
  mining.
\item Allow to handle the full notion of surprisingness, as will be
  further shown.
\item Offer more transparency. Produced knowledge can be reasoned
  upon. Reasoning steps taken during mining can be represented as data
  for subsequent mining and reasoning.
\item Enable meta-learning by leveraging URE's inference control
  mechanism.
\end{enumerate}
The last point, although already important as it stands, goes further
than that. One of the motivations to have a pattern miner in OpenCog
is to mine inference traces, and discover control rules from them to
speed-up reasoning in general. By framing not only pattern mining but
more generally learning as reasoning we hope to kickstart a virtuous
self-improvement cycle. Towards that end more components of OpenCog,
such as MOSES \cite{Looks06abstractcompetent}, an evolutionary program
learner, are currently in the process of being ported to the URE as
well.

Framing learning as reasoning is not without any drawback as more
transparency typically comes at a greater greater computational
cost. However by carefully partitioning transparent/costly versus
opaque/efficient computations we hope that an adequate balance between
efficiency and open-endedness can be achieved. For instance in the
case of evolutionary programming, decisions pertaining to what regions
of the program space to explore is best processed as reasoning, given
the importance and the cost of such operation. While more systematic
operations such as evaluating the fitness of a candidate can be left
as opaque computations. One may probably draw an analogy with the
distinction between conscious and unconscious processes.

\subsection{Outline}

In Section \ref{PMHD} a pattern mining algorithm over hypergraphs is
presented, and frame as reasoning in Section \ref{FPMR}. In Section
\ref{SUR} a definition of surprisingness is provided, a more
specialized definition is derived from it. Then an example of how it
can be framed as reasoning is presented, both for the specialized and
abstract definitions of surprisingness.

\section{Pattern Mining in Hypergraph Database}
\label{PMHD}

\subsection{AtomSpace: Hypegraph Database}

Let us first rapidly recall what is the AtomSpace
\cite{Goertzel2014}. The AtomSpace is OpenCog's primary data storage
infrastructure. It is a labeled hypergraph particularly suited for
representing symbolic knowledge, but is also capable of representing
sub-symbolic knowledge as well (probabilities, tensors, etc), and most
importantly combinations of the two. In the OpenCog terminology, the
edges of that hypergraph are called \emph{links}, the vertices are
called \emph{nodes}, and \emph{atoms} are either links or nodes.  For
example one may express that cars are vehicles with
\begin{verbatim}
(Inheritance (Concept "car") (Concept "vehicle"))
\end{verbatim}
If one wishes to express the other way around, how much vehicles are
cars, then one can label the inheritance link with a \emph{truth
  value}
\begin{verbatim}
(Inheritance (stv 0.4 0.8) (Concept "vehicle") (Concept "car"))
\end{verbatim}
where 0.4 represents a probability and 0.8 represents a confidence.

Storing knowledge as hypergraph rather than collections of formulae
allows to rapidly query atoms and how they relate to other atoms. For
instance given the node $\texttt{(Concept "car")}$ one can query the
links pointing to it, here two, one expressing that cars are vehicle,
the other expressing that some vehicles are cars.

\subsection{Pattern Matching}
\label{PM}

OpenCog comes with a \emph{pattern matcher}, a component that can
query the AtomSpace, similar in spirit to SQL, but different in
several aspects. For instance queries are themselves programs
represented as atoms in the AtomSpace. This insures reflexivity where
queries can be queried or produced by queries. Here's an example of
such query
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))
    (Inheritance (Variable "$Y") (Variable "$Z"))))
\end{verbatim}
which fetch any instance of transitivity of inheritance in the
AtomSpace. For instance if the AtomSpace contains
\begin{verbatim}
(Inheritance (Concept "cat") (Concept "mammal"))
(Inheritance (Concept "mammal") (Concept "animal"))
(Inheritance (Concept "square") (Concept "shape"))
\end{verbatim}
it retrieves
\begin{verbatim}
(Set
  (List (Concept "cat") (Concept "mammal") (Concept "animal")))
\end{verbatim}
where \texttt{cat}, \texttt{mammal} and \texttt{animal} are associated
to variable \texttt{\$X}, \texttt{\$Y} and \texttt{\$Z} according to
the prefix order of the query, but \texttt{square} and \texttt{shape}
are not retrieved because they do not exhibit transitivity. The
construct $\texttt{Set}$ represents a set of atoms, and
$\texttt{List}$ in this context represent tuples. The construct
\texttt{Get} means retrieve. The construct \texttt{Present} means that
the arguments are patterns to be conjunctively matched against the
data present in the AtomSpace. We also call \texttt{Present} arguments
\emph{clauses} and may say that the pattern is a \emph{conjunction of
  clauses}.

In addition, the pattern matcher can rewrite. For instance a
transitivity rule could be implemented with
\begin{verbatim}
(Bind
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))
    (Inheritance (Variable "$Y") (Variable "$Z")))
  (Inheritance (Variable "$X") (Variable "$Z")))
\end{verbatim}
The pattern matcher provides the building blocks for the reasoning
engine. In fact the URE is, for the most part, pattern matching +
unification.

\subsection{Pattern Mining as Inverse of Pattern Matching}

The pattern miner solves the inverse problem of pattern matcher. That
is provided data, it attempts to find queries that would retrieve a
certain \emph{minimum} number of matches. This number is called the
\emph{support} in the pattern mining terminology \cite{Chi2005Freq,
  Agrawal1994fastalgorithms}.

It is worth mentioning that the pattern matcher has many more
constructs than \texttt{Get}, \texttt{Present} and \texttt{Bind}; for
declaring types, expressing preconditions, and implementing pretty
much any computation one may want to express. However the pattern
miner only supports a subset of constructs due to the inherent
complexity of dealing with such expressiveness.

\subsection{High Level Algorithm of the Pattern Miner}

Before showing how to express pattern mining as reasoning, let us
explain the algorithm itself.

Our pattern mining algorithm operates like most pattern mining
algorithms \cite{Chi2005Freq} by searching the space of frequent
patterns while pruning the parts that do not have enough support. It
typically starts from the most abstract one, the \emph{top} pattern,
constructing specializations of it and only retain those that have
enough support, then repeat the specialization on the obtained
patterns and so on. The apriori property
\cite{Agrawal1994fastalgorithms} guaranties that no pattern with
enough support will be missed based on the fact that patterns without
enough support cannot have specializations with enough support.

Let us semi-formalize the above. Given a database $\mathcal{D}$, a
minimal support $S$ and an initialize collection $\mathcal{C}$ of
patterns with enough support, the mining algorithm operates as follows
\begin{enumerate}
\item Select a pattern $P$ from $\mathcal{C}$.
\item Produce a \emph{shallow specialization} $Q$ of $P$ with support
  equal to or above $S$.
\item Add $Q$ to $\mathcal{C}$, remove $P$ if all its shallow
  specializations have been produced.
\item Repeat till a termination criterion is met.
\end{enumerate}

The pattern collection $\mathcal{C}$ is usually initialized with the
top pattern
\begin{verbatim}
(Get
  (Present
    (Variable "$X")))
\end{verbatim}
that matches the whole database, and from which all subsequent
patterns are derived. A shallow specialization is a specialization
such that the expansion is only a level deep. For instance, if
$\mathcal{D}$ is the 3 inheritances links of Subsection \ref{PM} (cat
is a mammal, a mammal is an animal and square is a shape), a shallow
specialization of the top pattern could be
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))))
\end{verbatim}
which would match all inheritance links, thus have a support of 3. A
subsequent shallow specialization of it could be
\begin{verbatim}
(Get
  (Present
    (Inheritance (Concept "cat") (Variable "$Y"))))
\end{verbatim}
which would only match
\begin{verbatim}
(Inheritance (Concept "cat") (Concept "mammal"))
\end{verbatim}
and have a support of 1. So if the minimum support $S$ is 2, this one
would be discarded.

In practice the algorithm is complemented by heuristics to avoid an
exhaustive search, but that is the crux of it.

\section{Framing Pattern Mining as Reasoning}
\label{FPMR}

The hardest part of the algorithm above is step 1, selecting which
pattern to expand, which has the biggest impact on how the space is
being explored. When pattern mining is framed as reasoning such
decision corresponds to a \emph{premise or conclusion selection}.

TODO: add comment about the infix notation, EvaluationLink, etc.

Let us formalize the type of propositions we need to prove in order to
search the space of patterns. Given a database $\mathcal{D}$ and a
minimum support $S$ we want to instantiate and prove the following
theorems
$$ S \le \text{support}(P, \mathcal{D}) $$ expressing that pattern $P$
has enough support with respect to the data base $\mathcal{D}$, or to
simplify
$$ \text{minsup}(P, S, \mathcal{D}) := S \le \text{support}(P,
\mathcal{D}) $$
Then the primary inference rule we need is
$$ \text{minsup}(Q, S, \mathcal{D}) \land \text{spec}(Q, P) \vdash
\text{minsup}(P, S, \mathcal{D})$$ expressing that if $Q$ has enough
support, and $Q$ is a specialization of $P$, then $P$ has enough
support, essentially formalizing the apriori property. We can either
apply such rule in a forward way, from left to right, or in a backward
way, from right to left. If we search from more abstract to more
specialized then we want to use it in a backward way. Meaning the
reasoning engine needs to choose $P$ (conclusion selection from
$\text{minsup}(P, S, \mathcal{D})$) and then construct a
specialization $Q$.  In practice we actually have rewritten that rule
backwardly so that choosing $P$ actually amounts to a premise
selection. It is merely a technical detail, the idea is essentially
the same.

The definition of $\text{spec}$ is left out, but it is merely a
variation of the subtree relationship while accounting for
variables. As one can see a pattern such as
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))))
\end{verbatim}
is different than
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$X"))))
\end{verbatim}
where $\texttt{\$Y}$ has been replace by $\texttt{\$X}$, which
requires additional care.

Other \emph{heuristic} rules can be used to infer knowledge about
$\text{minsup}$. They are heuristics because unlike the apriori
property, they do not guaranty completeness, but can speed-up the
search by eliminating large portions of the search space. For instance
the following rule
$$ \text{minsup}(P, S, \mathcal{D}) \land \text{minsup}(Q, S,
\mathcal{D}) \land R(P \otimes Q) \vdash \text{minsup}(P \otimes Q, S,
\mathcal{D}) $$ expresses that if $P$ and $Q$ have enough support, and
a certain combination $P\otimes Q$ has a certain property $R$, then
such combination has enough support. Such rule type is used to combine
multiple conjunctions of patterns, that if for instance given $P$ and
$Q$ both being
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))))
\end{verbatim}
One can combine them to form
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))
    (Inheritance (Variable "$Y") (Variable "$Z"))))
\end{verbatim}
The property $R$ here is that both clauses must share at least one
joint variable and the combination must have its support above or
equal to the minimum threshold.

\section{Surprisingness}
\label{SUR}

Even with the help of the apriori property and additional heuristics
to prune the search, the volume of mined patterns can still be
overwhelming. For that it can be helpful to assign to the patterns a
measure of \emph{interestingness}. Such notion is broad and may mean
various things in various contexts. We will restrict our attention on
the sub-notion of \emph{surprisingness}. Although still broad one can
define surprisingness as what is \emph{contrary to expectations}. Such
notion is important as if some pattern contradicts expectations it
means we have uncovered an incorrectness or incompleteness in our
model of the world.

Just like for pattern mining, surprisingness can be framed as
reasoning. They are many ways to formalize such notion. We suggest
that in its most general sense, surpisingness may be the difference of
outcome between different inferences over the same conjecture. Of
course in most logical systems, if consistent, different inferences
will produce the same result. However in para-consistent systems, such
as PLN \cite{Goertzel2009PLN} for \emph{Probabilistic Logic Network},
OpenCog's logic for common sense reasoning, conflicting outcomes are
possible. In particular PLN allows a conjecture to be believed with
various degrees of truth, ranging from total ignorance to absolute
certainty. Thus PLN is well suited for such definition of
surprisingness. More specifically we define surprisingness as the
\emph{distance of truth values between different inferences over the
  same conjecture}. In PLN a truth value is a second order
distribution, probabilities over probabilities, as explained in
Chapter 4 of \cite{Goertzel2009PLN}. Second order distributions are
good at capturing uncertainties. Total ignorance is represented by a
flat distribution (Bayesian prior), or possibly a slightly concave one
(Jeffreys prior \cite{Jeffreys46Invariant}), and absolute certainty by
a Dirac delta function.

Such definition of surprisingness has the merit of encompassing a wide
variety of cases; like the surprisingness of finding a proof
contradicting human intuition. For instance the outcome of Euclid's
proof of the infinity of prime numbers might contradict the intuition
of a beginner upon observation that prime numbers rapidly rarefy as
numbers grow. It also encompasses the surprisingness of observing an
unexpected event, or the surprisingness of discovering a pattern in
seemingly random data. All these cases can be framed as ways of
constructing different types of inferences and finding contradictions
between them. For instance in the case of discovering a pattern in a
database, one inference could calculate the empirical probability
based on the data, while an other inference could calculate a
probability estimate based on the independence of the variables and
their marginal probabilities.

The distance measure to use to compare conjecture outcomes remains to
be defined. Since our truth values are distributions the
\emph{Jensen-Shannon Distance} (JSD for short) \cite{Endres2003A},
also suggested as surprisingness measure in \cite{Pienta2015AN,
  DerezinskiRH18}, could be used. The advantage of such distance is
that it accounts well for uncertainty. If for instance a pattern is
discovered in a small data set displaying high levels of dependencies
between variables (thus surprising relative to an independence
assumption of these variables), the surprisingness measure should
consider the possibility that it might be a fluke since the data set
is small. Fortunately, the smaller the data set, the flatter the
second order distributions representing the empirical and the
estimated truth values of the pattern, automatically reducing the JSD.

Likewise one can imagine the following coin tossing experiments. In
the first experiment a coin is tossed 3 times, a probability $p_1$ of
head is calculated, then the coin is tossed 3 more times, a second
probability $p_2$ of head is calculated. $p_1$ and $p_2$ might be very
different, but it should not be surprising given the low number of
observations. On the contrary, in the second experiment the coin is
tossed a billion times, $p_1$ is calculated, then another billion
times, $p_2$ is calculated. Here even tiny differences between $p_1$
and $p_2$ should be surprising. In both cases, by representing $p_1$
and $p_2$ as second order distributions rather than mere
probabilities, the Jensen-Shannon Distance properly accounts for the
uncertainty.

A slight refinement of our definition of surprisingness, probably
closer to human usage, can be obtained by fixing one type of inference
provided by the current model of the world from which rapid (and
usually uncertain) conclusions can be derived, and the other type of
inference implied by the world itself, either via observations, in the
case of an experienced reality, or via crisp and long chains of
deductions in the case of a mathematical reality. We will attempt to
model that refinement of surprisingness.

\subsection{Independence-based Surprisingness}
\label{IS}

Here we explore a limited form of surprisingness based on the
independence of the variables involved in clauses of a pattern, called
I-Surprisingness for Independence-based Surprisingness. For instance
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))
    (Inheritance (Variable "$Y") (Variable "$Z"))))
\end{verbatim}
has two clauses
\begin{verbatim}
(Inheritance (Variable "$X") (Variable "$Y"))
\end{verbatim}
and
\begin{verbatim}
(Inheritance (Variable "$Y") (Variable "$Z"))
\end{verbatim}
If each clause is considered independently, that is the distribution
of values taken by the variable tuples $(\texttt{\$X}, \texttt{\$Y})$
appearing in the first clause is independent from the distribution of
values taken by the variable tuples $(\texttt{\$Y}, \texttt{\$Z})$ in
the second clause, one can simply use the product of the two
distributions to obtain an estimate of the distribution of their
conjunctions. However the presence of joint variables, here
$\texttt{\$Y}$, makes this naive calculation wrong. Instead the
connections need to be taken into account. To do that we use the fact
that a pattern of connected clauses is equivalent to a pattern of
disconnected clauses combined with a condition of equality between the
joint variables. For instance
\begin{verbatim}
(Get
  (Present
    (Inheritance (Variable "$X") (Variable "$Y"))
    (Inheritance (Variable "$Y") (Variable "$Z"))))
\end{verbatim}
is equivalent to
\begin{verbatim}
(Get
  (And
    (Present
      (Inheritance (Variable "$X") (Variable "$Y1"))
      (Inheritance (Variable "$Y2") (Variable "$Z")))
    (Equal (Variable "$Y1") (Variable "$Y2"))))
\end{verbatim}
where the joint variables, here $\texttt{\$Y}$, have been replaced by
variable occurrences in each clause, $\texttt{\$Y1}$ and
$\texttt{\$Y2}$. The construct $\texttt{Equal}$ tests the equality of
values, and the construct $\texttt{And}$ is the logical
conjunction.
%% So the query means, find values matching these two
%% patterns, such that each value associated to $\texttt{\$Y1}$ is equal
%% to the value associated to $\texttt{\$Y2}$.
Then we can express the probability estimate as the product of the
probabilities of the clauses, as if they were independent, times the
probability of having the values of joint variables equal. The formula
estimating the probability of equality is not detailed here, but
according to our preliminary experiments, a decent estimate can be
obtained by performing some syntactic analysis as to take into account
specializations between clauses relative to their joint variables.
%% to estimate the number of value a particular variable occurrence
%% can take.  \footnote{More detail is provided in the code
%% documentation of
%% https://github.com/opencog/opencog/blob/master/opencog/learning/miner}.

\subsection{I-Surprisingness Framed as Reasoning and Beyond}

The expression to infer in order to calculate surprisingness is
defined as
$$
\text{surp}(P, \mathcal{D}, s)
$$
where $\texttt{surp}$ is a predicate relating $P$ and $\mathcal{D}$ to
its surprisingness $s$, defined as
$$
s := \text{dst}(\text{emp}(P,\mathcal{D}),\text{est}(P,\mathcal{D}))
$$ where $\text{dst}$ is the Jensen-Shannon distance, $\text{emp}$ is
the empirical second order distribution of $P$, and $\text{emp}$ its
estimate.

The calculation of $\text{emp}(P, \mathcal{D})$ is easily handled by a
\emph{direct evaluation} rule that uses the support of $P$ and the
size of $\mathcal{D}$ to obtain the parameters of the
beta-binomial-distribution describing the second order probability.
However, the mean by which the estimate is calculated is let
unspecified. This is up to the reasoning engine to find an inference
path to calculate it. Below is an example of inference tree to
calculate I-Surprisingness
\begin{prooftree}
  \AxiomC{$P$}
  \AxiomC{$\mathcal{D}$}

  \AxiomC{$P$}
  \AxiomC{$\mathcal{D}$}  
  \RightLabel{(DE)}
  \BinaryInfC{$\text{emp}(P,\mathcal{D})$}

  \AxiomC{$P$}
  \AxiomC{$\mathcal{D}$}  
  \RightLabel{(IS)}
  \BinaryInfC{$\text{est}(P,\mathcal{D})$}

  \RightLabel{(JSD)}
  \BinaryInfC{$\text{dst}(\text{emp}(P,\mathcal{D}),\text{est}(P,\mathcal{D}))$}  

  \RightLabel{(S)}
  \TrinaryInfC{$\text{surp}(P, \mathcal{D}, \text{dst}(\text{emp}(P,\mathcal{D}),\text{est}(P,\mathcal{D})))$}
\end{prooftree}
where
\begin{itemize}
\item (S) is a rule to construct the $\text{surp}$ predicate,
\item (JSD) is a rule to calculate the Jensen-Shannon Distance,
\item (DE) is the direct evaluation rule to calculate the empirical
  second order probability of $P$ according to $\mathcal{D}$,
\item (IS) is a rule to calculate the estimate of $P$ based on
  I-Surprisingness described in Section \ref{IS}.
\end{itemize}
That inference tree uses a single rule (IS) to calculate the
estimate. Most rules are fairly complex, such as (JSD)
%, possibly running C++ code for maximum efficiency
. So all that the URE must do
is put together such inference tree, which can be done reasonably well
given how much complexity is already encapsulated in the rules.

As of today we have only implemented (IS) for the estimate. In
general, however, we want to have more rules, and ultimately enough so
that the estimate can be inferred in an open-ended way. In such
scenario, the inference tree would look very similar to the one given
above, with the difference that the (IS) would be replaced by a
combination of other rules. Such approach would naturally lead to a
dynamic surprisingness measure. Indeed, inferring that some pattern is
I-Surprising requires to infer its estimate, and this knowledge can be
further utilized to infer estimates of related patterns. For instance,
let say an I-Surprising pattern is discovered about pets and food
intakes. A pattern about cats and food intakes might also be measured
as I-Surprising, however the fact that cat inherits pet may lead to
construct an inference that estimates the combination of cat and food
based on the combination of pet and food, possibly leading to a much
better estimate than using mere Independence, and thus decreasing the
surprisingness of that pattern.



%% some independence assumptions will be
%% defaulted new inference paths may be created from to produce better
%% estimates for other patterns, which would be then be inferred as less
%% surprising. For instance


%% including enough eventually  but in principle it could and should be prevents from letting
%% it open-ended It could be calculated using I-Surprisingness as
%% described above or via other types of inferences.


%% That is the URE calls that
%% rule after running the pattern miner, which itself calls the C++
%% code. This provides another example of how learning can be partitioned
%% into transparent reasoning and opaque computation.
%% $$
%% \text{surp}(P, \mathcal{D},
%% \text{dst}(\text{emp}(P,\mathcal{D}),\text{est}(P,\mathcal{D}))
%% $$

%% After discovering a few surprising patterns based on such independence
%% assumption, the data should be expected to contain such dependencies,
%% and subsequent estimates should take that into account when
%% calculating of the surprisingness of new patterns. For instance once
%% the empirical probability of a pattern is known, if such a pattern was
%% surprising such knowledge should be retained and utilized, so that any
%% other pattern that has its probability estimate rapidly derivable from
%% the previously surprising patterns (and other background knowledge),
%% and such that this estimate is good (the distance between the
%% empirical probability and the estimate is small) should not be
%% considered surprising.

%% For that the current I-Surprisingness rule needs to be decomposed so
%% that the estimate can be calculated in a separate inference, the
%% empirical probability in a separate inference, and the two compared in
%% a separated Jensen-Shannon distance step.

%% Let's denote this probability $P_e$. To estimate $P_e$ while avoiding
%% empirical knowledge about the interactions of the
%% clauses\footnote{$P_e$ can be estimated with the inner product of the
%%   distributions over the values the joint variable occurrences, but
%%   this captures interactions between the clauses exhibited in the
%%   data. We initially tried that and almost nothing was measured as
%%   surprising.} we estimate the probability of two variable occurrences
%% being equal by assuming values are uniformly distributed. Let us
%% denote $S(X_i)$ the number of values $X_i$ can take when its
%% corresponding clauses is matched in isolation. Thus, assuming $n$
%% clauses connected by variable $X$, an estimate would be
%% $$
%% P_e(X_1=...=X_n)~=\prod_{i=2}^n S(X_i)^{-1}
%% $$

%% The product ignores one of variable occurrence because for each value
%% we want to estimate the probability that the \emph{other} values equal
%% to this one. The quality of the estimate depends on the quality of the
%% estimations of $S(X_i)$. Without entering in too much details what we
%% have found is that by considering specialization relationships because
%% the clauses (captured by the \texttt{spec} predicate defined in
%% Section [REF]) we can place an upper bound on the number of values
%% that $S(X_i)$ can take, and provides a decent estimate.

%% In the end we obtain the following inference rule to calculate the
%% estimate
%% $$
%% P_e(X_1=...=X_n) = Prod_{i=2}^n M(X_i)^{-1}
%% $$
%% where $M(X_i)$ is either
%% \begin{enumerate}
%% \item $|V(X_j)|$, the number of values that $X_j$ can take in the most
%% specialized clause where $X_j$ occurs that is either more abstract
%% than or equivalent to the clauses where $X_i$ appears relative to $X$
%% and such that j<i.
%% \item $|U|$, the size of the data base if no such more abstract or
%%   equivalent component exists for $X_i$.
%% \end{enumerate}

%% This allows to see how such calculation can be entirely expressed as a
%% reasoning process, although in practice it is wrapped into a single
%% inference rule for efficiency purpose.

%% \subsection{Beyond Independence-based Surprisingness and Dynamic Measure}

%% Surprisingness has to be dynamic be nature, once a fact is known it is
%% no longer surprising, thus...

%% The advantage of framing measuring surprisingness as a reasoning
%% process is that we can easily make the transition from a hard coded
%% definition of surprisingness to a more general one. 

%% \section{Experiment: Mining Surprising Patterns in SUMO}

%% We have experiemented our current surprisingness measure on
%% synthesized data (to verify the correctness of the algorithm described
%% in REF), as well as real world data, in particular on the SUMO
%% ontology REF. Most surprising patterns discovered are rather abstract
%% and thus difficult to interpret, in spite of being surprising. Such
%% patterns are for instance

%% TODO

%% Also patterns that are concrete enough to be understood by a human,
%% such as

%% TODO (maritim)

%% are usually not surprising to a human because, although they are
%% indeed distant from their probability estimate under independence
%% assumptions, thus considered as surprising to the machine, are not
%% surprising to us because we can use our background knowledge of the
%% world as well as our capacity to infer a better estimate to be able to
%% not be surprised.

%% \section{Conclusion}

%% It is clear from that experiment that the next step is to enable some
%% form of open-ended (yet reasonably fast) means of inferences as to
%% take into account more semantic knowledge, and thus .

%
% ---- Bibliography ----
%
\bibliographystyle{splncs04}
\bibliography{my}

\end{document}
